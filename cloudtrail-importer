#!/bin/env python
"""
Extracts logfiles from AWS Cloudtrail and pipes into Logstash via Redis.

Config file is JSON format, similar options as logstash itself.  Sample:

{
    "input": [
        {
            "type":         "cloudtrail",
            "bucket":       "ec2-accounting",
            "account":      "acctnum",
            "aws_role":     "CloudtrailProcessor",
            "add_field":    {
                "environment": "dev"
            }
        }
    ],

    "output": [
        {
            "type":     "redis",
            "host":     "localhost",
            "key":      "syslogs"
        }
    ]
}

Input options:
    type:       logstash 'type' field, for sorting logfile type.  Must be 'cloudtrail'.
    add_field:  optional hash of fields + arguments to add to the message
    aws_role:   Optional IAM instance-profile role (preferred)
    aws_key:    Optional AWS access key
    aws_secret: Optional AWS secret key (used with 'aws_key')
    account:    AWS account number
    bucket:     S3 bucket where logs are stored

Output options:
    type:       'redis' or 'stdout'
    host:       Redis host (required for redis)
    port:       Redis port (optional)
    key:        Name of the redis list used for this log (required for redis)

Packages required: redis, boto
"""

__author__    = 'Michael Stella <michael@jwplayer.com>'
__copyright__ = "Copyright (c) 2014 Long Tail Ad Solutions"
__version__   = "1.2"

import argparse
import datetime
import gzip
import json
import logging
import os
import socket
import sys

import boto
from boto.sts import STSConnection

import logstash

S3PATH = 'AWSLogs/{acct}/CloudTrail/us-east-1/{year}/{month:02d}/{day:02d}/'
METADATA = 'http://169.254.169.254/latest/meta-data/iam/security-credentials/'

logging.basicConfig(level=logging.WARN, format='%(asctime)s %(levelname)-8s %(name)-6s %(message)s')
log_input=logging.getLogger('input')
log_output=logging.getLogger('output')

inputs = []
outputs = []
hostname = None

class CloudtrailInput(object):
    """Cloudtrail input handling class"""

    type = 'cloudtrail'

    def __init__(self, account, bucket, auth, fields={}):
        self.account = account
        self.bucket = bucket
        self.fields = fields

        self.conn = boto.connect_s3(**auth)


    def get_files(self, date):
        """Get all files in the bucket/prefix"""

        prefix = S3PATH.format(acct=self.account, year=date.year, month=date.month, day=date.day)
        log_input.info("Reading s3 bucket: s3://{}/{}".format(self.bucket, prefix))

        bucket = self.conn.get_bucket(self.bucket)

        for key in bucket.list(prefix):
            log_input.debug(" reading file {}".format(key.name))

            try:
                # download the file into a tempfile
                tmpname = '/tmp/{}'.format(os.path.basename(key.name))
                with open(tmpname , 'wb') as f:
                    key.get_file(f)

                # send the downloaded logfile to processing
                self.parse_file(tmpname)

                # remove temporary file
                if os.path.exists(tmpname):
                    os.unlink(tmpname)

            except boto.exception.S3ResponseError as e:
                log_input.error("{} when reading file {} - skipping".format(e.message, key.name))


    def parse_file(self, fn):
        """Process the downloaded logfile"""
        opener = open
        if fn.endswith('gz'):
            opener = gzip.open

        with opener(fn, 'rb') as f:
            data = json.loads(f.read())
            for rec in data['Records']:
                rec.update({
                    'type': self.type,
                })

                # add fields from the config file
                for k,v in self.fields.items():
                    if not k in rec:
                        rec[k] = v

                # send the message to all outputs
                for o in outputs:
                    o.log(**rec)


def get_auth(account, role):
    """Obtain a temporary authentication key via STS.
    This assumes your server is running with an instance role that allows
    'AssumeRoles' permission.
    Provide the role name, this returns a dictionary with these keys:
        (aws_access_key_id, aws_secret_access_key, security_token)"""

    stsc = STSConnection()
    assumedRoleObject = stsc.assume_role(
        role_arn='arn:aws:iam::{}:role/{}'.format(account, role),
        role_session_name='cloudtrail-importer')

    return {
        'aws_access_key_id':        assumedRoleObject.credentials.access_key,
        'aws_secret_access_key':    assumedRoleObject.credentials.secret_key,
        'security_token':           assumedRoleObject.credentials.session_token,
    }


def init(args):
    """Setup"""
    (cfginputs, cfgoutputs) = logstash.read_config(args.cfgfile[0])

    # connect to outputs
    for o in cfgoutputs:
        if o['type'] == 'redis':
            try:
                r = logstash.RedisSink(o['host'], o['key'])
                outputs.append(r)
                log_output.debug("Connected to redis server '{0}'".format(o['host']))

            except Exception as e:
                log_output.error("Redis: {0}".format(e))

        elif o['type'] == 'stdout':
            outputs.append(logstash.StdoutSink())

    for i in cfginputs:
        if i['type'] != 'cloudtrail':
            continue

        auth = {}
        # first try command-line AWS credential overrides
        if args.aws_key and args.aws_secret:
            auth = {
                'aws_access_key_id': args.aws_key,
                'aws_secret_access_key': args.aws_secret
            }

        # next, command-line provided IAM role
        elif args.aws_role:
            try:
                auth = get_auth(i['account'], args.aws_role)
            except Exception as e:
                log_input.error("Error fetching AWS role '{}' for account {}: {}".format(args.aws_role, i['account'], e))
                sys.exit(-1)

        # if there's an aws_role specified, try to query the
        # ec2 metadata server for temporary credentials
        elif 'aws_role' in i:
            try:
                auth = get_auth(i['account'], i['aws_role'])
            except Exception as e:
                log_input.error("Error fetching AWS role '{}' for account {}: {}".format(i['aws_role'], i['account'], e))
                sys.exit(-1)

        # otherwise, credentials must be provided in the config file
        else:
            if ('aws_key' not in i) or ('aws_secret' not in i):
                log_output.error("Cannot find AWS credentials - check config or provide on command-line")
                sys.exit(-1)
            auth = {
                'aws_access_key_id':     i['aws_key'],
                'aws_secret_access_key': i['aws_secret'],
            }

        ctinput = CloudtrailInput(i['account'], i['bucket'], auth)
        if 'add_field' in i:
            ctinput.fields = i['add_field']

        inputs.append(ctinput)


    if len(inputs) < 1:
        logging.error("No inputs, exiting")
        sys.exit(-1)

    if len(outputs) < 1:
        logging.error("No outputs, exiting")
        sys.exit(-1)


if __name__ == '__main__':

    today = datetime.date.today()

    argparser = argparse.ArgumentParser()

    # aws security
    argparser.add_argument('--aws_key', type=str, help="AWS Access Key (override config file)")
    argparser.add_argument('--aws_secret', type=str, help="AWS Secret Key (override config file)")
    argparser.add_argument('--aws_role', type=str, help="IAM role in target accounts")

    argparser.add_argument('--date', type=str, default=today.strftime('%Y-%m-%d'), help="Date as YYYY-MM-DD")
    argparser.add_argument('--debug', action='store_true', default=False)
    argparser.add_argument('cfgfile', nargs=1, help="JSON-format config file")

    opts = argparser.parse_args(sys.argv[1:])

    if opts.debug:
        log_input.setLevel(logging.DEBUG)
        log_output.setLevel(logging.DEBUG)

    hostname = socket.gethostname()
    conn = init(opts)


    input_date = datetime.datetime.strptime(opts.date, '%Y-%m-%d').date()
    for inp in inputs:
        inp.get_files(input_date)

